{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07380f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize_scalar\n",
    "import support_functions as sf\n",
    "import statistical_analysis as sa\n",
    "import importlib, support_functions, inspect, statistical_analysis\n",
    "importlib.reload(support_functions)\n",
    "importlib.reload(statistical_analysis)\n",
    "# print('module file:', support_functions.__file__)\n",
    "# print(inspect.getsource(support_functions.change_product_name)[:400])\n",
    "# from pygam import glm, s, f, te, PoissonGAM, ExponentialGAM, ExpectileGAM\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "path = Path(\"data\") / \"ml_task_data.csv\"\n",
    "raw_data = pd.read_csv(path)\n",
    "# raw_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5659cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>margin</th>\n",
       "      <th>sales</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>product_A</td>\n",
       "      <td>82.39</td>\n",
       "      <td>24.72</td>\n",
       "      <td>19</td>\n",
       "      <td>1565.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>product_C</td>\n",
       "      <td>65.89</td>\n",
       "      <td>19.77</td>\n",
       "      <td>8</td>\n",
       "      <td>527.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>product_B</td>\n",
       "      <td>36.19</td>\n",
       "      <td>10.86</td>\n",
       "      <td>6</td>\n",
       "      <td>217.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>product_D</td>\n",
       "      <td>43.89</td>\n",
       "      <td>13.17</td>\n",
       "      <td>1</td>\n",
       "      <td>43.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>product_C</td>\n",
       "      <td>65.89</td>\n",
       "      <td>19.77</td>\n",
       "      <td>26</td>\n",
       "      <td>1713.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>product_C</td>\n",
       "      <td>71.39</td>\n",
       "      <td>21.42</td>\n",
       "      <td>81</td>\n",
       "      <td>5782.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>product_D</td>\n",
       "      <td>49.39</td>\n",
       "      <td>14.82</td>\n",
       "      <td>19</td>\n",
       "      <td>938.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>product_A</td>\n",
       "      <td>98.89</td>\n",
       "      <td>29.67</td>\n",
       "      <td>53</td>\n",
       "      <td>5241.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>product_B</td>\n",
       "      <td>38.39</td>\n",
       "      <td>11.52</td>\n",
       "      <td>70</td>\n",
       "      <td>2687.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>product_E</td>\n",
       "      <td>219.89</td>\n",
       "      <td>65.97</td>\n",
       "      <td>278</td>\n",
       "      <td>61129.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3553 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           product_id  sell_price  margin  sales   revenue\n",
       "date                                                      \n",
       "2019-01-01  product_A       82.39   24.72     19   1565.41\n",
       "2019-01-01  product_C       65.89   19.77      8    527.12\n",
       "2019-01-01  product_B       36.19   10.86      6    217.14\n",
       "2019-01-01  product_D       43.89   13.17      1     43.89\n",
       "2019-01-02  product_C       65.89   19.77     26   1713.14\n",
       "...               ...         ...     ...    ...       ...\n",
       "2021-01-27  product_C       71.39   21.42     81   5782.59\n",
       "2021-01-27  product_D       49.39   14.82     19    938.41\n",
       "2021-01-27  product_A       98.89   29.67     53   5241.17\n",
       "2021-01-27  product_B       38.39   11.52     70   2687.30\n",
       "2021-01-27  product_E      219.89   65.97    278  61129.42\n",
       "\n",
       "[3553 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data\n",
    "df, df_product_A, df_product_B, df_product_C, df_product_D, df_product_E = support_functions.rawDataReorganise(raw_data)\n",
    "# quick check of products\n",
    "# df['product_id'].value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5a30a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id\n",
       "product_A    755\n",
       "product_B    739\n",
       "product_C    737\n",
       "product_D    677\n",
       "product_E    645\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b3ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first and last day of sales for each product\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "\n",
    "sales_date_range = []\n",
    "for product, df_prod in zip(products, dfs):\n",
    "    first_day = df_prod.index.min()\n",
    "    last_day = df_prod.index.max()\n",
    "    total_days = (last_day - first_day).days + 1\n",
    "    days_with_sales = len(df_prod)\n",
    "    days_no_sales = total_days - days_with_sales\n",
    "    sales_date_range.append({\n",
    "        'Product': product,\n",
    "        'First Sale Date': first_day,\n",
    "        'Last Sale Date': last_day,\n",
    "        'Total Days': total_days,\n",
    "        'Days with Sales': days_with_sales,\n",
    "        'Days with No Sales': days_no_sales\n",
    "    })\n",
    "\n",
    "sales_range_df = pd.DataFrame(sales_date_range)\n",
    "print(\"Sales Date Range by Product:\")\n",
    "print(sales_range_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find first and last week of sales for each product\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "\n",
    "sales_week_range = []\n",
    "for product, df_prod in zip(products, dfs):\n",
    "    # Extract year and week number\n",
    "    df_prod_copy = df_prod.copy()\n",
    "    df_prod_copy['year_week'] = df_prod_copy.index.isocalendar().year.astype(str) + '-W' + df_prod_copy.index.isocalendar().week.astype(str).str.zfill(2)\n",
    "    \n",
    "    unique_weeks = df_prod_copy['year_week'].unique()\n",
    "    first_week = unique_weeks[0]\n",
    "    last_week = unique_weeks[-1]\n",
    "    weeks_with_sales = len(unique_weeks)\n",
    "    \n",
    "    # Calculate total weeks from first to last\n",
    "    first_date = df_prod.index.min()\n",
    "    last_date = df_prod.index.max()\n",
    "    total_weeks = ((last_date - first_date).days // 7) + 1\n",
    "    weeks_no_sales = total_weeks - weeks_with_sales\n",
    "    \n",
    "    sales_week_range.append({\n",
    "        'Product': product,\n",
    "        'First Sales Week': first_week,\n",
    "        'Last Sales Week': last_week,\n",
    "        'Total Weeks': total_weeks,\n",
    "        'Weeks with Sales': weeks_with_sales,\n",
    "        'Weeks with No Sales': weeks_no_sales\n",
    "    })\n",
    "\n",
    "sales_week_df = pd.DataFrame(sales_week_range)\n",
    "print(\"Sales Week Range by Product:\")\n",
    "print(sales_week_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc978750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot weekly sales scatter for all products in the same plot (with calendar weeks - quarter markers)\n",
    "def plot_weekly_sales_scatter(product_dfs, product_names):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of weekly sales for all products in the same plot using calendar weeks.\n",
    "    Only shows quarter start weeks on x-axis.\n",
    "    \n",
    "    Parameters:\n",
    "    product_dfs: list of product dataframes\n",
    "    product_names: list of product names/letters\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    colors = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    # Collect all weeks to determine unique x-axis positions\n",
    "    all_weeks = []\n",
    "    weekly_data = {}\n",
    "    \n",
    "    for product, df_prod, color, marker in zip(product_names, product_dfs, colors, markers):\n",
    "        # Extract year-week and sum sales\n",
    "        df_copy = df_prod.copy()\n",
    "        df_copy['year_week'] = df_copy.index.isocalendar().year.astype(str) + '-W' + df_copy.index.isocalendar().week.astype(str).str.zfill(2)\n",
    "        \n",
    "        weekly_sales = df_copy.groupby('year_week')[f'sales_{product}'].sum().reset_index()\n",
    "        weekly_data[product] = weekly_sales\n",
    "        all_weeks.extend(weekly_sales['year_week'].tolist())\n",
    "    \n",
    "    # Get unique weeks in order\n",
    "    unique_weeks = sorted(set(all_weeks))\n",
    "    week_to_index = {week: idx for idx, week in enumerate(unique_weeks)}\n",
    "    \n",
    "    # Plot each product\n",
    "    for product, df_prod, color, marker in zip(product_names, product_dfs, colors, markers):\n",
    "        weekly_sales = weekly_data[product]\n",
    "        x_positions = [week_to_index[week] for week in weekly_sales['year_week']]\n",
    "        \n",
    "        ax.scatter(x_positions, np.log(weekly_sales[f'sales_{product}']), \n",
    "                  label=f'Product {product}', alpha=0.7, s=100, color=color, marker=marker)\n",
    "    \n",
    "    # Extract quarter start weeks (weeks 1, 14, 27, 40)\n",
    "    quarter_weeks = []\n",
    "    quarter_positions = []\n",
    "    quarter_labels = []\n",
    "    \n",
    "    for week, idx in week_to_index.items():\n",
    "        week_num = int(week.split('-W')[1])\n",
    "        if week_num in [1, 14, 27, 40]:\n",
    "            quarter_weeks.append(week)\n",
    "            quarter_positions.append(idx)\n",
    "            quarter_labels.append(week)\n",
    "    \n",
    "    # Set x-axis with only quarter start week labels\n",
    "    ax.set_xticks(quarter_positions)\n",
    "    ax.set_xticklabels(quarter_labels, rotation=45, ha='right')\n",
    "    ax.set_xlim(-1, len(unique_weeks))\n",
    "    ax.set_xlabel('Calendar Week (Quarter Starts)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Log Weekly Sales', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Log Weekly Sales Scatter - All Products (Calendar Weeks - Quarters)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='best')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "plot_weekly_sales_scatter(dfs, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fdec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create continuous week column for df_product_E (based on elapsed time from earliest date)\n",
    "min_date = df_product_E.index.min()\n",
    "df_product_E['week'] = np.ceil((df_product_E.index - min_date).days / 7).astype(int)\n",
    "# Ensure week starts at 1 (not 0)\n",
    "df_product_E['week'] = df_product_E['week'] + 1\n",
    "print(f\"Week column created. Min date: {min_date}, Unique weeks: {sorted(df_product_E['week'].unique())}\")\n",
    "# print(df_product_E[['week']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d62b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd84073",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2a859",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4072c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support_functions import change_column_names\n",
    "df_product_A =change_column_names(df_product_A, 'A')\n",
    "df_product_B =change_column_names(df_product_B, 'B')\n",
    "df_product_C =change_column_names(df_product_C, 'C')\n",
    "df_product_D =change_column_names(df_product_D, 'D')\n",
    "df_product_E =change_column_names(df_product_E, 'E')\n",
    "# Show the first rows and columns to confirm\n",
    "df_product_A.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_product_A.merge(df_product_B, left_index=True, right_index=True, how='outer') \\\n",
    "              .merge(df_product_C, left_index=True, right_index=True, how='outer') \\\n",
    "                  .merge(df_product_D, left_index=True, right_index=True, how='outer') \\\n",
    "                      .merge(df_product_E, left_index=True, right_index=True, how='outer')\n",
    "                      \n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe8017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432db7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = sf.create_temporal_features(df_merge)\n",
    "# df_merge.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93862ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = sa.log_correlation_plot(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071683cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dependent_cols = [\n",
    "    'log_sales_A', 'log_sales_B', 'log_sales_C',\n",
    "    'log_sales_D', 'log_sales_E'\n",
    "]\n",
    "# df_corr.loc['log_sales_A'][np.abs(df_corr.loc['log_sales_A']) > 0.5].drop('log_sales_A')\n",
    "for col in log_dependent_cols:\n",
    "    corr_cols = df_corr.loc[col][np.abs(df_corr.loc[col]) > 0.5].drop(col)\n",
    "    print(f\"Columns correlated with {col} (|corr| > 0.5):\")\n",
    "    if len(corr_cols) == 0:\n",
    "        print(f\"  {col} is not correlated with any of the given values\")\n",
    "    else:\n",
    "        for c, val in corr_cols.items():\n",
    "            print(f\"  {c}: {val:.4f}\")\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0cdac",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_statsmodels(fdf, product ='A',other_products_flag = False  ,temporal_columns_flag=False):\n",
    "    \"\"\"\n",
    "    Perform OLS regression using statsmodels to predict log_sales_{product} from df_model.\n",
    "    \"\"\"\n",
    "    temporal_col_names = [ 'dayofmonth', 'month', 'year','is_weekend'] #'dayofweek',\n",
    "    # 'dayofyear' and 'month' are correlated - exclude 'dayofyear'\n",
    "    col_name_product = [f'log_sales_{product}']\n",
    "    col_name_product_price = [f'log_sell_price_{product}']\n",
    "    col_names_price = [ 'log_sell_price_A','log_sell_price_B','log_sell_price_C','log_sell_price_D','log_sell_price_E']\n",
    "    if other_products_flag == False:\n",
    "        col_names_price = col_name_product_price\n",
    "    if temporal_columns_flag == False:\n",
    "        temporal_col_names = []\n",
    "    df_model = fdf[col_name_product+col_names_price+temporal_col_names].copy(deep=True)\n",
    "    print(f\"Running OLS regression for product: {product}\")\n",
    "    print(\"-\"*70)\n",
    "    print(df_model.columns.tolist())\n",
    "    # Use df_model\n",
    "    df_work = df_model.copy(deep=True)\n",
    "\n",
    "    target = f'log_sales_{product}'\n",
    "    if target not in df_work.columns:\n",
    "        raise KeyError(f\"{target} not found in df_model columns: {df_work.columns.tolist()}\")\n",
    "\n",
    "    # Select numeric features (exclude target)\n",
    "    X = df_work.select_dtypes(include=[\"number\"]).drop(columns=[target], errors='ignore')\n",
    "    y = df_work[target]\n",
    "\n",
    "    # Drop rows with NaNs\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X = X.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # Remove constant columns\n",
    "    X = X.loc[:, X.std() > 0]\n",
    "\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Number of observations: {len(y)}\")\n",
    "    print(f\"Features ({len(X.columns)}): {X.columns.tolist()}\")\n",
    "\n",
    "    # Fit OLS\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_sm)\n",
    "    results = model.fit()\n",
    "    beta0, beta1 = results.params[0], results.params[1]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATSMODELS OLS REGRESSION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary())\n",
    "\n",
    "    # Store results\n",
    "    _sm_ols_results = results\n",
    "    \n",
    "    return _sm_ols_results, beta0, beta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = linear_regression_statsmodels(df_merge, product ='A')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defa8b2",
   "metadata": {},
   "source": [
    "We have used **power-law demand function:**\n",
    "$$ ln(q) = \\beta_0 + \\beta_1 ln(p) $$\n",
    "$$ q(p) = e^{\\beta_0} e^{\\beta_1 ln(p)} = e^{\\beta_0} e^{ln(p^{\\beta_1})} = e^{\\beta_0} p^{\\beta1} $$\n",
    "$$R(p) = p \\times q(p) = e^{\\beta_0} \\times p^{\\beta_1 + 1}$$\n",
    "Using FOC (to maximize revenue):\n",
    "$$\\frac{dR}{dp} = 0 \\implies \\frac{e^{\\beta_0} \\times p^{\\beta_1 + 1}}{dp} = 0$$\n",
    "$$\\implies \\beta_1 = -1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_merge[['sell_price_A','sales_A','log_sales_A','log_sell_price_A']].copy(deep=True)\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        'sell_price_A':'price','sales_A':'sales',\n",
    "        'log_sell_price_A':'ln_price','log_sales_A':'ln_sales'\n",
    "        }\n",
    "    )\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "# Fit log–log regression\n",
    "X = sm.add_constant(data['ln_price'])\n",
    "model = sm.OLS(data['ln_sales'], X).fit()\n",
    "beta0, beta1 = model.params\n",
    "\n",
    "print(\"Regression coefficients:\")\n",
    "print(f\"β0 = {beta0:.4f}, β1 = {beta1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand function\n",
    "def demand(p):\n",
    "    return np.exp(beta0) * p**beta1\n",
    "\n",
    "# Revenue function\n",
    "def revenue(p):\n",
    "    return p * demand(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec30580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maximize revenue numerically within a sensible range\n",
    "res = minimize_scalar(lambda p: -revenue(p),\n",
    "                      bounds=(min(data['price']), max(data['price'])*5),\n",
    "                      method='bounded')\n",
    "\n",
    "optimal_price = res.x\n",
    "max_revenue = revenue(optimal_price)\n",
    "\n",
    "print(f\"Optimal price: {optimal_price:.2f}\")\n",
    "print(f\"Max revenue: {max_revenue:.2f}\")\n",
    "print(f\"Sales: {max_revenue/optimal_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_statsmodels(df_merge, product ='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_statsmodels(df_merge, product ='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_statsmodels(df_merge, product ='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ec832",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_statsmodels(df_merge, product ='E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean regression workflow: impute, scale, Lasso feature selection, sklearn LR, and statsmodels OLS\n",
    "# # Requires: scikit-learn, statsmodels (already installed)\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LassoCV, LinearRegression\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import r2_score\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Use merged dataframe\n",
    "# df_work = df_model.copy()\n",
    "\n",
    "# target = 'log_sales_A'\n",
    "# if target not in df_work.columns:\n",
    "#     raise KeyError(f\"{target} not found in df_merge columns: {df_work.columns.tolist()}\")\n",
    "\n",
    "# # Select numeric features (exclude target)\n",
    "# X_raw = df_work.select_dtypes(include=[\"number\"]).drop(columns=[target], errors='ignore')\n",
    "# y = df_work[target]\n",
    "\n",
    "# # Align and drop rows where target is missing\n",
    "# mask = y.notna()\n",
    "# X_raw = X_raw.loc[mask]\n",
    "# y = y.loc[mask]\n",
    "\n",
    "# # Impute median for numeric features\n",
    "# imp = SimpleImputer(strategy='median')\n",
    "# X_imp = pd.DataFrame(imp.fit_transform(X_raw), index=X_raw.index, columns=X_raw.columns)\n",
    "\n",
    "# # Remove constant columns\n",
    "# stds = X_imp.std()\n",
    "# non_const_cols = stds[stds > 0].index.tolist()\n",
    "# X_imp = X_imp[non_const_cols]\n",
    "\n",
    "# # Scale\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = pd.DataFrame(scaler.fit_transform(X_imp), index=X_imp.index, columns=X_imp.columns)\n",
    "\n",
    "# # LassoCV for feature selection (use default 5-fold CV)\n",
    "# lasso = LassoCV(n_alphas=100, cv=5, random_state=42, max_iter=5000)\n",
    "# lasso.fit(X_scaled, y.loc[X_scaled.index])\n",
    "# selector = SelectFromModel(lasso, prefit=True, threshold='median')\n",
    "# selected_mask = selector.get_support()\n",
    "# selected_features = X_scaled.columns[selected_mask].tolist()\n",
    "\n",
    "# print('Selected features ({}):'.format(len(selected_features)))\n",
    "# print(selected_features)\n",
    "\n",
    "# if len(selected_features) == 0:\n",
    "#     print('No features selected by Lasso; using top 10 by absolute coef')\n",
    "#     coef_series = pd.Series(np.abs(lasso.coef_), index=X_scaled.columns)\n",
    "#     selected_features = list(coef_series.sort_values(ascending=False).head(10).index)\n",
    "\n",
    "# X_sel = X_scaled[selected_features]\n",
    "\n",
    "# # Train/test evaluation with sklearn LinearRegression\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_sel, y.loc[X_sel.index], test_size=0.2, random_state=42)\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_pred = lr.predict(X_test)\n",
    "# print('\\nSklearn LinearRegression R^2 on test set:', r2_score(y_test, y_pred))\n",
    "\n",
    "# # Show top coefficients\n",
    "# coefs = pd.Series(lr.coef_, index=selected_features)\n",
    "# print('\\nTop coefficients (sklearn LR):')\n",
    "# print(coefs.sort_values(key=lambda s: s.abs(), ascending=False).head(20))\n",
    "\n",
    "# # Statsmodels OLS on selected features (full data without scaling inverse is fine for inference here)\n",
    "# # Rebuild X for statsmodels using original (imputed but not scaled) values for interpretability\n",
    "# X_sm = X_imp[selected_features]\n",
    "# X_sm = sm.add_constant(X_sm)\n",
    "# model = sm.OLS(y.loc[X_sm.index], X_sm)\n",
    "# results = model.fit()\n",
    "# print('\\nOLS summary:')\n",
    "# print(results.summary())\n",
    "\n",
    "# # Store results objects for later inspection\n",
    "# _regression_results = {\n",
    "#     'selected_features': selected_features,\n",
    "#     'sklearn_lr': lr,\n",
    "#     'lasso_model': lasso,\n",
    "#     'statsmodels_results': results\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b073ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_price_sales(fdf, product_name):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    print(\"Here\")\n",
    "\n",
    "    # Plot sales over time\n",
    "    axes[0].plot(fdf.index, fdf[f'log_sales_{product_name}'], marker='o', linestyle='-', linewidth=2)\n",
    "    axes[0].set_title(f'{product_name} - Log Sales Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Log Sales')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot sell_price over time\n",
    "    axes[1].plot(fdf.index, fdf[f'log_sell_price_{product_name}'], marker='s', linestyle='-', linewidth=2, color='orange')\n",
    "    axes[1].set_title(f'{product_name} - Log Sell Price Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Log Sell Price')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b03100",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_price_sales(df_merge, 'A')\n",
    "# plot_price_sales(df_merge, 'B')\n",
    "# plot_price_sales(df_merge, 'C')\n",
    "# plot_price_sales(df_merge, 'D')\n",
    "plot_price_sales(df_merge, 'E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaceb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log sales for all products\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "\n",
    "for idx, (product, df_prod) in enumerate(zip(products, dfs)):\n",
    "    axes[idx].plot(df_prod.index, df_prod[f'log_sales_{product}'], marker='o', linestyle='-', linewidth=2, color=f'C{idx}')\n",
    "    axes[idx].set_title(f'Product {product} - Log Sales Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Log Sales')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log sales for all products - 2019 only\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "\n",
    "for idx, (product, df_prod) in enumerate(zip(products, dfs)):\n",
    "    df_2019 = df_prod[df_prod.index.year == 2019]\n",
    "    axes[idx].plot(df_2019.index, df_2019[f'log_sales_{product}'], marker='o', linestyle='-', linewidth=2, color=f'C{idx}')\n",
    "    axes[idx].set_title(f'Product {product} - Log Sales Over Time (2019)', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Log Sales')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log sales for all products - 2020 only\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "products = ['A', 'B', 'C', 'D', 'E']\n",
    "dfs = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "\n",
    "for idx, (product, df_prod) in enumerate(zip(products, dfs)):\n",
    "    df_2020 = df_prod[df_prod.index.year == 2020]\n",
    "    axes[idx].plot(df_2020.index, df_2020[f'log_sales_{product}'], marker='o', linestyle='-', linewidth=2, color=f'C{idx}')\n",
    "    axes[idx].set_title(f'Product {product} - Log Sales Over Time (2020)', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Log Sales')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21970775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_sales_correlation(df_product, product_name):\n",
    "    # Correlation plot between sell_price and sales\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.scatter(df_product['log_price'], df_product['log_sales'], alpha=0.6, s=50, color='steelblue')\n",
    "    ax.set_xlabel('Log Sell Price', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Log Sales', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{product_name} - Correlation between Log Sell Price and Log Sales', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr = df_product['log_price'].corr(df_product['log_sales'])\n",
    "    ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax.transAxes, \n",
    "            fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_price_sales_correlation(df_product_A, 'Product A')\n",
    "# plot_price_sales_correlation(df_product_B, 'Product B')\n",
    "# plot_price_sales_correlation(df_product_C, 'Product C')\n",
    "# plot_price_sales_correlation(df_product_D, 'Product D')\n",
    "plot_price_sales_correlation(df_product_E, 'Product E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Day-wise boxplot of sales\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# products = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "# product_names = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n",
    "# days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# for idx, (df_product, product_name) in enumerate(zip(products, product_names)):\n",
    "#     # Prepare data for boxplot\n",
    "#     data_by_day = [df_product[df_product['dayofweek'] == day]['sales'].values for day in range(7)]\n",
    "    \n",
    "#     bp = axes[idx].boxplot(data_by_day, labels=days, patch_artist=True)\n",
    "    \n",
    "#     # Color the boxes\n",
    "#     for patch in bp['boxes']:\n",
    "#         patch.set_facecolor('lightblue')\n",
    "    \n",
    "#     axes[idx].set_title(f'{product_name} - Day-wise Sales Distribution', fontsize=11, fontweight='bold')\n",
    "#     axes[idx].set_xlabel('Day of Week')\n",
    "#     axes[idx].set_ylabel('Sales')\n",
    "#     axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# # Hide the extra subplot\n",
    "# axes[5].set_visible(False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f09837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Day-wise boxplot of sell_price\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# products = [df_product_A, df_product_B, df_product_C, df_product_D, df_product_E]\n",
    "# product_names = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n",
    "# days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# for idx, (df_product, product_name) in enumerate(zip(products, product_names)):\n",
    "#     # Prepare data for boxplot\n",
    "#     data_by_day = [df_product[df_product['dayofweek'] == day]['sell_price'].values for day in range(7)]\n",
    "    \n",
    "#     bp = axes[idx].boxplot(data_by_day, labels=days, patch_artist=True)\n",
    "    \n",
    "#     # Color the boxes\n",
    "#     for patch in bp['boxes']:\n",
    "#         patch.set_facecolor('lightcoral')\n",
    "    \n",
    "#     axes[idx].set_title(f'{product_name} - Day-wise Price Distribution', fontsize=11, fontweight='bold')\n",
    "#     axes[idx].set_xlabel('Day of Week')\n",
    "#     axes[idx].set_ylabel('Sell Price')\n",
    "#     axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# # Hide the extra subplot\n",
    "# axes[5].set_visible(False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add45ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Requirements: statsmodels, scikit-learn\n",
    "# # pip install statsmodels scikit-learn\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Replace with your merged dataframe variable if different\n",
    "# df_work = df_merge.copy()\n",
    "\n",
    "# target = 'log_sales_A'   # dependent variable\n",
    "# if target not in df_work.columns:\n",
    "#     raise KeyError(f\"{target} not found in dataframe columns: {df_work.columns.tolist()}\")\n",
    "\n",
    "# # Build X: numeric columns except the target\n",
    "# X = df_work.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
    "# y = df_work[target]\n",
    "\n",
    "# # Drop rows with NaNs in X or y\n",
    "# mask = X.notna().all(axis=1) & y.notna()\n",
    "# X = X.loc[mask]\n",
    "# y = y.loc[mask]\n",
    "\n",
    "# # (Optional) remove perfectly collinear or constant columns\n",
    "# X = X.loc[:, X.std() > 0]\n",
    "\n",
    "# # --- OLS (statsmodels) ---\n",
    "# X_sm = sm.add_constant(X)         # adds intercept term\n",
    "# model = sm.OLS(y, X_sm)\n",
    "# results = model.fit()\n",
    "# print(\"OLS summary:\")\n",
    "# print(results.summary())\n",
    "\n",
    "# # --- Train/test linear regression (scikit-learn) ---\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_pred = lr.predict(X_test)\n",
    "# print(\"\\nSklearn LinearRegression R^2 on test set:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# # Show coefficients (aligned to feature names)\n",
    "# coefs = pd.Series(lr.coef_, index=X.columns)\n",
    "# print(\"\\nCoefficients (sklearn LR):\")\n",
    "# print(coefs.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39edf8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compact regression summary, save full OLS to file, and run CV (R^2 and MAE)\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "# import numpy as np\n",
    "\n",
    "# out_dir = Path('results')\n",
    "# out_dir.mkdir(exist_ok=True)\n",
    "# summary_path = out_dir / 'regression_summary.txt'\n",
    "\n",
    "# # Retrieve stored results\n",
    "# res = globals().get('_regression_results', None)\n",
    "# if res is None:\n",
    "#     raise RuntimeError(\"_regression_results not found. Run the regression cell first.\")\n",
    "\n",
    "# selected_features = res.get('selected_features', [])\n",
    "# lr = res.get('sklearn_lr')\n",
    "# lasso = res.get('lasso_model')\n",
    "# sm_res = res.get('statsmodels_results')\n",
    "\n",
    "# # Compact print\n",
    "# print('Selected features ({}):'.format(len(selected_features)))\n",
    "# print(selected_features)\n",
    "\n",
    "# # Top coefficients from sklearn LR\n",
    "# if lr is not None:\n",
    "#     coefs = pd.Series(lr.coef_, index=selected_features)\n",
    "#     print('\\nTop coefficients (sklearn LR):')\n",
    "#     print(coefs.sort_values(key=lambda s: s.abs(), ascending=False).head(10))\n",
    "# else:\n",
    "#     print('\\nNo sklearn LR model found in _regression_results')\n",
    "\n",
    "# # Test R^2: try to compute if test split stored (not stored). We'll recompute a quick train/test split for a compact R^2.\n",
    "# try:\n",
    "#     X_imp = globals().get('X_imp')\n",
    "#     y = globals().get('y')\n",
    "#     if X_imp is None or y is None:\n",
    "#         # try to rebuild minimal X_imp,y from df_merge\n",
    "#         df_work = globals().get('df_merge')\n",
    "#         if df_work is None:\n",
    "#             raise RuntimeError('df_merge not found for recomputing test R^2')\n",
    "#         # use selected features from df_merge (impute median)\n",
    "#         X_temp = df_work[selected_features].copy()\n",
    "#         X_temp = X_temp.fillna(X_temp.median())\n",
    "#         y_temp = df_work['log_sales_A'].loc[X_temp.index]\n",
    "#     else:\n",
    "#         X_temp = X_imp[selected_features].copy()\n",
    "#         y_temp = y.loc[X_temp.index]\n",
    "\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "#     lr_tmp = LinearRegression()\n",
    "#     lr_tmp.fit(X_train, y_train)\n",
    "#     y_pred = lr_tmp.predict(X_test)\n",
    "#     test_r2 = r2_score(y_test, y_pred)\n",
    "#     print(f\"\\nCompact test R^2 (recomputed): {test_r2:.4f}\")\n",
    "# except Exception as e:\n",
    "#     print('\\nCould not recompute compact test R^2:', e)\n",
    "\n",
    "# # Cross-validated scores\n",
    "# try:\n",
    "#     X_cv = X_temp\n",
    "#     y_cv = y_temp\n",
    "#     # R^2 CV\n",
    "#     cv_r2 = cross_val_score(LinearRegression(), X_cv, y_cv, cv=5, scoring='r2')\n",
    "#     # MAE CV (negative MAE returned by scikit-learn with scoring that expects higher is better)\n",
    "#     neg_mae = cross_val_score(LinearRegression(), X_cv, y_cv, cv=5, scoring='neg_mean_absolute_error')\n",
    "#     cv_mae = -neg_mae\n",
    "#     print(f\"\\nCV R^2 (5-fold): mean={cv_r2.mean():.4f}, std={cv_r2.std():.4f}\")\n",
    "#     print(f\"CV MAE (5-fold): mean={cv_mae.mean():.4f}, std={cv_mae.std():.4f}\")\n",
    "# except Exception as e:\n",
    "#     print('\\nCV evaluation failed:', e)\n",
    "\n",
    "# # Save full OLS summary to file\n",
    "# try:\n",
    "#     with open(summary_path, 'w') as f:\n",
    "#         f.write('Selected features:\\n')\n",
    "#         f.write('\\n'.join(selected_features) + '\\n\\n')\n",
    "#         f.write('Sklearn coefficients (top 50):\\n')\n",
    "#         if lr is not None:\n",
    "#             coef_lines = coefs.sort_values(key=lambda s: s.abs(), ascending=False).head(50).to_string()\n",
    "#             f.write(coef_lines + '\\n\\n')\n",
    "#         f.write('Statsmodels OLS summary:\\n')\n",
    "#         if sm_res is not None:\n",
    "#             f.write(str(sm_res.summary()) + '\\n')\n",
    "#         else:\n",
    "#             f.write('No statsmodels results found')\n",
    "#     print(f\"\\nFull OLS summary saved to {summary_path}\")\n",
    "# except Exception as e:\n",
    "#     print('\\nFailed to write summary file:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b205f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multicollinearity check: correlation pairs, VIF, and condition number\n",
    "# from pathlib import Path\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# product = 'A'\n",
    "# temporal_col_names =['dayofweek', 'dayofmonth','dayofyear', 'month', 'year','is_weekend']\n",
    "# col_name_product = [f'log_sales_{product}']\n",
    "# col_names_price = [ 'log_sell_price_A','log_sell_price_B','log_sell_price_C','log_sell_price_D','log_sell_price_E']\n",
    "# df_model = df_merge[col_name_product+col_names_price+temporal_col_names].copy(deep=True)\n",
    "# out_dir = Path('results')\n",
    "# out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Choose working dataframe: df_model (as built earlier)\n",
    "# df_work = globals().get('df_model', None)\n",
    "# if df_work is None:\n",
    "#     raise RuntimeError('df_model not found in notebook namespace')\n",
    "\n",
    "# # target may be log_sales_A — exclude target from features\n",
    "# target = f'log_sales_{product}'\n",
    "# if target in df_work.columns:\n",
    "#     X = df_work.drop(columns=[target])\n",
    "# else:\n",
    "#     X = df_work.copy()\n",
    "\n",
    "# # Keep numeric columns only\n",
    "# X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "# if X_num.shape[1] == 0:\n",
    "#     raise RuntimeError('No numeric features found to compute multicollinearity')\n",
    "\n",
    "# # Impute median for any missing values to allow VIF computation\n",
    "# imp = SimpleImputer(strategy='median')\n",
    "# X_imp = pd.DataFrame(imp.fit_transform(X_num), index=X_num.index, columns=X_num.columns)\n",
    "\n",
    "# # Compute correlation matrix and list high-correlation pairs\n",
    "# corr = X_imp.corr()\n",
    "# threshold = 0.8\n",
    "# high_pairs = []\n",
    "# cols = X_imp.columns.tolist()\n",
    "# for i in range(len(cols)):\n",
    "#     for j in range(i+1, len(cols)):\n",
    "#         v = corr.iloc[i, j]\n",
    "#         if abs(v) >= threshold:\n",
    "#             high_pairs.append((cols[i], cols[j], float(v)))\n",
    "\n",
    "# # Save correlation matrix and high pairs\n",
    "# corr.to_csv(out_dir / 'correlation_matrix.csv')\n",
    "# with open(out_dir / 'high_corr_pairs.txt', 'w') as f:\n",
    "#     f.write(f'Pairs with |corr| >= {threshold}\\n')\n",
    "#     for a,b,v in high_pairs:\n",
    "#         f.write(f'{a}\\t{b}\\t{v:.4f}\\n')\n",
    "\n",
    "# # Prepare X for VIF: add constant not needed for VIF calculation\n",
    "# # Remove constant / zero-variance columns\n",
    "# stds = X_imp.std()\n",
    "# non_const = stds[stds > 0].index.tolist()\n",
    "# X_vif = X_imp[non_const]\n",
    "\n",
    "# # Standardize before computing condition number\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = pd.DataFrame(scaler.fit_transform(X_vif), index=X_vif.index, columns=X_vif.columns)\n",
    "\n",
    "# # Compute VIFs\n",
    "# vifs = []\n",
    "# for i, col in enumerate(X_vif.columns):\n",
    "#     try:\n",
    "#         vif_val = variance_inflation_factor(X_vif.values, i)\n",
    "#     except Exception as e:\n",
    "#         vif_val = np.nan\n",
    "#     vifs.append((col, float(vif_val)))\n",
    "\n",
    "# vif_df = pd.DataFrame(vifs, columns=['feature', 'VIF']).sort_values('VIF', ascending=False)\n",
    "# vif_df.to_csv(out_dir / 'vif.csv', index=False)\n",
    "\n",
    "# # Condition number (of X'X)\n",
    "# cond_number = np.linalg.cond(X_scaled)\n",
    "\n",
    "# # Compact printout\n",
    "# print('Multicollinearity check — compact summary')\n",
    "# print('-'*60)\n",
    "# print(f'Number of numeric features: {X_num.shape[1]}')\n",
    "# print(f'High-correlation pairs (|corr| >= {threshold}): {len(high_pairs)}')\n",
    "# for a,b,v in high_pairs[:10]:\n",
    "#     print(f'  {a} — {b}: {v:.3f}')\n",
    "# if len(high_pairs) > 10:\n",
    "#     print(f'  ... and {len(high_pairs)-10} more')\n",
    "\n",
    "# print('\\nTop VIFs:')\n",
    "# print(vif_df.head(10).to_string(index=False))\n",
    "# print(f'\\nCondition number (scaled X): {cond_number:.3f}')\n",
    "# print('\\nSaved:')\n",
    "# print(f' - correlation matrix -> {out_dir / \"correlation_matrix.csv\"}')\n",
    "# print(f' - high correlation pairs -> {out_dir / \"high_corr_pairs.txt\"}')\n",
    "# print(f' - VIFs -> {out_dir / \"vif.csv\"}')\n",
    "\n",
    "# # Expose variables for later inspection\n",
    "# _vif_df = vif_df\n",
    "# _high_corr_pairs = high_pairs\n",
    "# _cond_number = cond_number\n",
    "# _corr_matrix = corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43442d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot for df_merge['log_sell_price_E']\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure column exists and drop NA values\n",
    "col = 'log_sell_price_E'\n",
    "if col not in df_merge.columns:\n",
    "    raise KeyError(f\"{col} not found in df_merge columns: {df_merge.columns.tolist()}\")\n",
    "\n",
    "data = df_merge[col].dropna()\n",
    "if data.shape[0] == 0:\n",
    "    print(f'No non-null values found for {col}')\n",
    "else:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.kdeplot(data, fill=True, bw_method='scott', color='C0')\n",
    "    plt.title('Density plot: log_sell_price_E', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('log_sell_price_E')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0c559",
   "metadata": {},
   "source": [
    "# Product E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f877099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_E.dropna(inplace=True)\n",
    "df_product_E['revenue_E'] = df_product_E['sell_price_E'] * df_product_E['sales_E']\n",
    "df_product_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a86cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_E = df_product_E['log_sell_price_E'].mode()\n",
    "print(type(mode_E))\n",
    "print(mode_E.iloc[0] )\n",
    "df_product_E['flag_discount_E'] = np.where(\n",
    "    df_product_E['log_sell_price_E'] < mode_E.iloc[0]*0.98, 1, 0\n",
    ")\n",
    "df_product_E['percentage_discount_E'] =  (mode_E.iloc[0] - df_product_E['log_sell_price_E']) / mode_E.iloc[0] * 100\n",
    "df_product_E['percentage_discount_E'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65319be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create continuous week column for df_product_E (based on elapsed time from earliest date)\n",
    "min_date = df_product_E.index.min()\n",
    "df_product_E['week'] = np.ceil((df_product_E.index - min_date).days / 7).astype(int)\n",
    "# Ensure week starts at 1 (not 0)\n",
    "df_product_E['week'] = df_product_E['week'] + 1\n",
    "print(f\"Week column created. Min date: {min_date}, Unique weeks: {sorted(df_product_E['week'].unique())}\")\n",
    "# print(df_product_E[['week']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72beebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e581a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['sell_price_E','sales_E','log_sales_E','log_sell_price_E','flag_discount_E','week']\n",
    "data = df_product_E[col_names].copy(deep=True)\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        'sell_price_E':'price','sales_E':'sales',\n",
    "        'log_sell_price_E':'ln_price','log_sales_E':'ln_sales'\n",
    "        }\n",
    "    )\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "# Fit log–log regression\n",
    "X = sm.add_constant(data['week'])\n",
    "model = sm.OLS(data['ln_sales'], X).fit()\n",
    "beta0, beta1 = model.params\n",
    "\n",
    "print(\"Regression coefficients:\")\n",
    "print(f\"β0 = {beta0:.4f}, β1 = {beta1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_weekend'] = (data['dayofweek'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_statsmodels(fdf, target_column_name, independent_variables_columns):\n",
    "    \"\"\"\n",
    "    Perform OLS regression using statsmodels to predict log_sales_{product} from df_model.\n",
    "    \"\"\"\n",
    "    df_model = fdf[[target_column_name] + independent_variables_columns].copy(deep=True)\n",
    "    print(df_model.columns )\n",
    "    print(\"-\"*70)\n",
    "    print(df_model.columns.tolist())\n",
    "    # Use df_model\n",
    "    df_work = df_model.copy(deep=True)\n",
    "\n",
    "    target = target_column_name\n",
    "    if target not in df_work.columns:\n",
    "        raise KeyError(f\"{target} not found in df_model columns: {df_work.columns.tolist()}\")\n",
    "\n",
    "    # Select numeric features (exclude target)\n",
    "    X = df_work.select_dtypes(include=[\"number\"]).drop(columns=[target], errors='ignore')\n",
    "    y = df_work[target]\n",
    "\n",
    "    # Drop rows with NaNs\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X = X.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # Remove constant columns\n",
    "    X = X.loc[:, X.std() > 0]\n",
    "\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Number of observations: {len(y)}\")\n",
    "    print(f\"Features ({len(X.columns)}): {X.columns.tolist()}\")\n",
    "\n",
    "    # Fit OLS\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_sm)\n",
    "    results = model.fit()\n",
    "    beta0, beta1 = results.params[0], results.params[1]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATSMODELS OLS REGRESSION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.summary())\n",
    "\n",
    "    # Store results\n",
    "    _sm_ols_results = results\n",
    "    \n",
    "    return _sm_ols_results, beta0, beta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ba58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables_columns = ['week','percentage_discount_E']\n",
    "target_column_name = 'log_sales_E'\n",
    "linear_regression_statsmodels(df_product_E, target_column_name, independent_variables_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables_columns = ['week','flag_discount_E']\n",
    "target_column_name = 'log_sales_E'\n",
    "linear_regression_statsmodels(df_product_E, target_column_name, independent_variables_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables_columns = ['week','log_sell_price_E']\n",
    "target_column_name = 'log_sales_E'\n",
    "linear_regression_statsmodels(df_product_E, target_column_name, independent_variables_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_E.groupby('week')['sales_E'].sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_E.groupby('week')['sell_price_E'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_A['sell_price_A'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f96e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64027550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6a067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
